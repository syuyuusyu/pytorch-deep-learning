{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output,display\n",
    "#https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/\n",
    "device = 'mps'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "all_data = ImageFolder(root='food-101/images/',transform=transform)\n",
    "\n",
    "train_size =int( 0.8 * len(all_data))\n",
    "test_size = len(all_data) - train_size\n",
    "\n",
    "train_data,test_data = random_split(all_data,[train_size,test_size])\n",
    "train_data,test_data = train_data.dataset,test_data.dataset\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=128,shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data,batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomModelWithConv(nn.Module):\n",
    "    def __init__(self, weight_decay=1e-4,dropout_prob=0.5):\n",
    "        super(CustomModelWithConv, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(10 * 16 * 16, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(32, len(train_data.classes)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 添加L2正则化\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def l2_regularization(self):\n",
    "        l2_reg = torch.tensor(0.0).to(device)\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) > 1:  # 仅对权重矩阵应用 L2 正则化\n",
    "                l2_reg += torch.norm(param, p='fro')  # 计算 Frobenius 范数\n",
    "        return self.weight_decay * l2_reg\n",
    "    \n",
    "module = CustomModelWithConv()\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "module.apply(weights_init)\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "from torch.optim.lr_scheduler import StepLR,ReduceLROnPlateau\n",
    "#optimizer = torch.optim.Adam(params=module.parameters(),lr = 0.01, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adadelta(module.parameters(), lr=0.05, rho=0.9, eps=1e-6, weight_decay=1e-4)\n",
    "optimizer = torch.optim.SGD(params=module.parameters(),lr = 0.01)\n",
    "#scheduler = StepLR(optimizer, step_size=20, gamma=0.2)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "from util import accuracy_fn,train_step,test_step\n",
    "\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience= 10\n",
    "current_patience = 0\n",
    "epochs = 100\n",
    "train_result = []\n",
    "test_result = []\n",
    "\n",
    "test_losses= []\n",
    "test_accs = []\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(3, 2, figsize=(16, 8), gridspec_kw={'height_ratios': [1, 1,1]})\n",
    "#plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss,train_acc,train_epoch_losses,trian_epoch_accs = train_step(module=module,data_loader=train_loader,loss_fn=loss_fn,optimizer=optimizer,device=device,accuracy_fn=accuracy_fn)\n",
    "    test_loss,test_acc,test_epoch_losses,test_epoch_accs = test_step(module=module,data_loader= test_loader,loss_fn= loss_fn,device=device,accuracy_fn=accuracy_fn)\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        current_patience = 0\n",
    "    else:\n",
    "        current_patience += 1\n",
    "\n",
    "    train_result.append((epoch,train_loss,train_acc))\n",
    "    train_losses.append(train_loss.detach().cpu().numpy())\n",
    "    #train_losses.append(train_loss.cpu())\n",
    "    train_accs.append(train_acc)\n",
    "    test_result.append((epoch,test_loss,test_acc))\n",
    "    test_losses.append(test_loss.detach().cpu().numpy())\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    ax1.plot([item.detach().cpu().numpy() for item in train_epoch_losses])\n",
    "    ax2.plot([item.detach().cpu().numpy() for item in test_epoch_losses])\n",
    "    ax3.plot(train_losses, )\n",
    "    ax4.plot(test_losses, )\n",
    "    ax5.plot(train_accs)\n",
    "    ax6.plot(test_accs)\n",
    "    ax1.set_title(f'current epoch:{epoch},Train Loss')\n",
    "    ax2.set_title(f'current epoch:{epoch},Test Loss')\n",
    "    ax3.set_title(f'Train Loss={[ np.round(item,4) for item in train_losses[-5:]]}')\n",
    "    ax4.set_title(f'Test Loss={[ np.round(item,4) for item in test_losses[-5:]]}')\n",
    "    ax5.set_title(f'Trian Accuracy={[ round(item,4) for item in train_accs[-5:]]}')\n",
    "    ax6.set_title(f'Test Accuracy={[ round(item,4) for item in test_accs[-5:]]}')\n",
    "    display(fig)\n",
    "    end_time = time.time()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'-------epoch------:{epoch} time:{(end_time - start_time):.2f}s current_lr:{current_lr},current_patience:{current_patience}')\n",
    "    for name, param in module.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(f'Gradient for {name}: {param.grad.norm()}')\n",
    "    if current_patience == patience:\n",
    "        print(f'Early stopping! No improvement for {patience} consecutive epochs.')\n",
    "        print(f\"train loss:{[ np.round(item,4) for item in train_losses]}\")\n",
    "        print(f\"test loss:{[ np.round(item,4) for item in test_losses]}\")\n",
    "        break\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"last 10 loss:{[ np.round(item,4) for item in test_losses[-10:]]}\")\n",
    "plt.ioff()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
